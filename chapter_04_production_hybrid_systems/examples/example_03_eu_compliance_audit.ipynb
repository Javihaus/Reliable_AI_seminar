{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: EU AI Act Compliance Framework\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/agents_observability_bootcamp/blob/main/chapter_04_production_hybrid_systems/examples/example_03_eu_compliance_audit.ipynb)\n",
    "\n",
    "**Instructor demonstration** - Students follow along\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Demonstrate EU AI Act compliant audit logging and transparency features.\n",
    "\n",
    "**Key lesson**: Compliance is engineering, not just legal. Build it into your system architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "import uuid\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EU AI Act Compliant Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EUAIActLogger:\n",
    "    \"\"\"EU AI Act Articles 13, 15, 17 compliant audit logger\"\"\"\n",
    "    \n",
    "    def __init__(self, system_name, system_version):\n",
    "        self.system_name = system_name\n",
    "        self.system_version = system_version\n",
    "        self.audit_log = []\n",
    "        self.risk_events = []\n",
    "        \n",
    "    def log_decision(self, decision_data):\n",
    "        \"\"\"\n",
    "        Log AI decision (Article 15: Logging and traceability)\n",
    "        \n",
    "        Required fields:\n",
    "        - Unique decision ID\n",
    "        - Timestamp\n",
    "        - Input/output (hashed for privacy)\n",
    "        - Processing method\n",
    "        - Confidence score\n",
    "        - Explanation (Article 13: Transparency)\n",
    "        \"\"\"\n",
    "        decision_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Hash sensitive data (GDPR compatibility)\n",
    "        input_hash = hashlib.sha256(\n",
    "            str(decision_data.get('input', '')).encode()\n",
    "        ).hexdigest()[:16]\n",
    "        \n",
    "        output_hash = hashlib.sha256(\n",
    "            str(decision_data.get('output', '')).encode()\n",
    "        ).hexdigest()[:16]\n",
    "        \n",
    "        log_entry = {\n",
    "            # Article 15: Traceability\n",
    "            'decision_id': decision_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'system_name': self.system_name,\n",
    "            'system_version': self.system_version,\n",
    "            \n",
    "            # Decision details (privacy-preserving)\n",
    "            'input_hash': input_hash,\n",
    "            'output_hash': output_hash,\n",
    "            'processing_method': decision_data.get('processing_method', 'unknown'),\n",
    "            'model_used': decision_data.get('model', 'unknown'),\n",
    "            \n",
    "            # Article 13: Transparency\n",
    "            'confidence': decision_data.get('confidence', 0.0),\n",
    "            'explanation': decision_data.get('explanation', ''),\n",
    "            'ai_system_used': True,\n",
    "            \n",
    "            # Validation results\n",
    "            'validation_passed': decision_data.get('validation_passed', False),\n",
    "            'validation_checks': decision_data.get('validation_checks', []),\n",
    "            \n",
    "            # User context\n",
    "            'user_id_hash': hashlib.sha256(\n",
    "                str(decision_data.get('user_id', '')).encode()\n",
    "            ).hexdigest()[:16],\n",
    "            'session_id': decision_data.get('session_id', '')\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(log_entry)\n",
    "        return decision_id\n",
    "    \n",
    "    def log_risk_event(self, event_data):\n",
    "        \"\"\"\n",
    "        Log risk event (Article 17: Quality management)\n",
    "        \n",
    "        Risk events include:\n",
    "        - System errors\n",
    "        - Unexpected behavior\n",
    "        - Performance degradation\n",
    "        - Compliance violations\n",
    "        \"\"\"\n",
    "        event = {\n",
    "            'event_id': str(uuid.uuid4()),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'severity': event_data.get('severity', 'medium'),\n",
    "            'category': event_data.get('category', 'unknown'),\n",
    "            'description': event_data.get('description', ''),\n",
    "            'affected_decisions': event_data.get('affected_decisions', []),\n",
    "            'mitigation_action': event_data.get('mitigation_action', '')\n",
    "        }\n",
    "        \n",
    "        self.risk_events.append(event)\n",
    "        return event['event_id']\n",
    "    \n",
    "    def get_decision_trail(self, decision_id):\n",
    "        \"\"\"Retrieve complete audit trail for specific decision (Article 15)\"\"\"\n",
    "        for entry in self.audit_log:\n",
    "            if entry['decision_id'] == decision_id:\n",
    "                return entry\n",
    "        return None\n",
    "    \n",
    "    def generate_compliance_report(self):\n",
    "        \"\"\"\n",
    "        Generate compliance report for regulatory review\n",
    "        (Articles 13, 15, 17)\n",
    "        \"\"\"\n",
    "        total_decisions = len(self.audit_log)\n",
    "        \n",
    "        # Analyze processing methods\n",
    "        methods = defaultdict(int)\n",
    "        for entry in self.audit_log:\n",
    "            methods[entry['processing_method']] += 1\n",
    "        \n",
    "        # Analyze validation rates\n",
    "        validated = sum(1 for e in self.audit_log if e['validation_passed'])\n",
    "        \n",
    "        # Analyze confidence scores\n",
    "        confidences = [e['confidence'] for e in self.audit_log]\n",
    "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "        \n",
    "        report = {\n",
    "            'report_date': datetime.now().isoformat(),\n",
    "            'system_name': self.system_name,\n",
    "            'system_version': self.system_version,\n",
    "            \n",
    "            # Article 15: Logging statistics\n",
    "            'total_decisions_logged': total_decisions,\n",
    "            'decisions_by_method': dict(methods),\n",
    "            'validation_pass_rate': validated / total_decisions if total_decisions > 0 else 0,\n",
    "            'average_confidence': avg_confidence,\n",
    "            \n",
    "            # Article 13: Transparency compliance\n",
    "            'explanations_provided': sum(1 for e in self.audit_log if e['explanation']),\n",
    "            'explanation_rate': sum(1 for e in self.audit_log if e['explanation']) / total_decisions if total_decisions > 0 else 0,\n",
    "            \n",
    "            # Article 17: Risk management\n",
    "            'total_risk_events': len(self.risk_events),\n",
    "            'risk_events_by_severity': self._count_by_field(self.risk_events, 'severity'),\n",
    "            \n",
    "            # Retention compliance\n",
    "            'retention_period': '7 years',\n",
    "            'data_encryption': 'SHA-256 hashing applied',\n",
    "            'gdpr_compatible': True\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _count_by_field(self, items, field):\n",
    "        \"\"\"Helper to count items by field value\"\"\"\n",
    "        counts = defaultdict(int)\n",
    "        for item in items:\n",
    "            counts[item.get(field, 'unknown')] += 1\n",
    "        return dict(counts)\n",
    "    \n",
    "    def export_audit_log(self, filepath='audit_log.json'):\n",
    "        \"\"\"Export audit log (Article 15: Accessibility to regulators)\"\"\"\n",
    "        export_data = {\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'system_name': self.system_name,\n",
    "            'system_version': self.system_version,\n",
    "            'total_entries': len(self.audit_log),\n",
    "            'audit_log': self.audit_log,\n",
    "            'risk_events': self.risk_events\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        \n",
    "        return filepath\n",
    "\n",
    "print(\"EUAIActLogger defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Compliance in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize compliant logger\n",
    "logger = EUAIActLogger(\n",
    "    system_name=\"Hybrid Content Moderation System\",\n",
    "    system_version=\"v1.0.0\"\n",
    ")\n",
    "\n",
    "print(\"EU AI Act Compliant Logger initialized\\n\")\n",
    "\n",
    "# Simulate decisions\n",
    "decisions = [\n",
    "    {\n",
    "        'input': 'Is this content appropriate?',\n",
    "        'output': 'safe',\n",
    "        'processing_method': 'deterministic',\n",
    "        'model': 'rule-based',\n",
    "        'confidence': 0.95,\n",
    "        'explanation': 'Content matches safe patterns with high confidence',\n",
    "        'validation_passed': True,\n",
    "        'validation_checks': ['keyword_check', 'length_check'],\n",
    "        'user_id': 'user_123',\n",
    "        'session_id': 'session_abc'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Complex borderline content',\n",
    "        'output': 'safe_with_warning',\n",
    "        'processing_method': 'llm',\n",
    "        'model': 'claude-sonnet-4',\n",
    "        'confidence': 0.72,\n",
    "        'explanation': 'Content requires nuanced interpretation but deemed acceptable',\n",
    "        'validation_passed': True,\n",
    "        'validation_checks': ['llm_validation', 'human_review_flag'],\n",
    "        'user_id': 'user_456',\n",
    "        'session_id': 'session_def'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Obvious spam content',\n",
    "        'output': 'spam',\n",
    "        'processing_method': 'deterministic',\n",
    "        'model': 'rule-based',\n",
    "        'confidence': 0.98,\n",
    "        'explanation': 'Multiple spam indicators detected',\n",
    "        'validation_passed': True,\n",
    "        'validation_checks': ['spam_filter', 'keyword_check'],\n",
    "        'user_id': 'user_789',\n",
    "        'session_id': 'session_ghi'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Logging decisions...\\n\")\n",
    "for i, decision in enumerate(decisions, 1):\n",
    "    decision_id = logger.log_decision(decision)\n",
    "    print(f\"Decision {i} logged: {decision_id}\")\n",
    "    print(f\"  Method: {decision['processing_method']}\")\n",
    "    print(f\"  Output: {decision['output']}\")\n",
    "    print(f\"  Confidence: {decision['confidence']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Risk Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate risk events\n",
    "risk_events = [\n",
    "    {\n",
    "        'severity': 'low',\n",
    "        'category': 'performance',\n",
    "        'description': 'LLM latency exceeded 3s threshold',\n",
    "        'affected_decisions': [],\n",
    "        'mitigation_action': 'Monitoring continued, no action required'\n",
    "    },\n",
    "    {\n",
    "        'severity': 'medium',\n",
    "        'category': 'accuracy',\n",
    "        'description': 'Deterministic rule disagreement rate increased to 8%',\n",
    "        'affected_decisions': [],\n",
    "        'mitigation_action': 'Rule review scheduled, LLM fallback activated'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Logging risk events...\\n\")\n",
    "for event in risk_events:\n",
    "    event_id = logger.log_risk_event(event)\n",
    "    print(f\"Risk event logged: {event_id}\")\n",
    "    print(f\"  Severity: {event['severity']}\")\n",
    "    print(f\"  Category: {event['category']}\")\n",
    "    print(f\"  Description: {event['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Compliance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EU AI ACT COMPLIANCE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "report = logger.generate_compliance_report()\n",
    "print(json.dumps(report, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLIANCE CHECKLIST\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Article 13: Transparency\n",
    "print(\"Article 13 - Transparency:\")\n",
    "print(f\"  ✓ AI system usage disclosed: {report['total_decisions_logged']} decisions\")\n",
    "print(f\"  ✓ Explanations provided: {report['explanation_rate']:.1%} of decisions\")\n",
    "print(f\"  ✓ Confidence scores recorded: {report['average_confidence']:.2f} average\")\n",
    "print()\n",
    "\n",
    "# Article 15: Logging\n",
    "print(\"Article 15 - Logging and Traceability:\")\n",
    "print(f\"  ✓ All decisions logged: {report['total_decisions_logged']} entries\")\n",
    "print(f\"  ✓ Unique decision IDs: Generated for each decision\")\n",
    "print(f\"  ✓ Timestamps recorded: ISO 8601 format\")\n",
    "print(f\"  ✓ Processing methods tracked: {list(report['decisions_by_method'].keys())}\")\n",
    "print(f\"  ✓ Retention period: {report['retention_period']}\")\n",
    "print()\n",
    "\n",
    "# Article 17: Quality Management\n",
    "print(\"Article 17 - Quality Management:\")\n",
    "print(f\"  ✓ Risk events monitored: {report['total_risk_events']} events logged\")\n",
    "print(f\"  ✓ Validation tracking: {report['validation_pass_rate']:.1%} pass rate\")\n",
    "print(f\"  ✓ Incident reporting: System operational\")\n",
    "print()\n",
    "\n",
    "# GDPR Compatibility\n",
    "print(\"GDPR Compatibility:\")\n",
    "print(f\"  ✓ PII protection: {report['data_encryption']}\")\n",
    "print(f\"  ✓ Privacy-preserving: Input/output hashed\")\n",
    "print(f\"  ✓ GDPR compatible: {report['gdpr_compatible']}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Specific Decision Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Regulatory inquiry for specific decision\n",
    "if logger.audit_log:\n",
    "    sample_decision_id = logger.audit_log[0]['decision_id']\n",
    "    \n",
    "    print(f\"Retrieving audit trail for decision: {sample_decision_id}\\n\")\n",
    "    \n",
    "    trail = logger.get_decision_trail(sample_decision_id)\n",
    "    \n",
    "    if trail:\n",
    "        print(\"Complete Audit Trail:\")\n",
    "        print(json.dumps(trail, indent=2))\n",
    "    else:\n",
    "        print(\"Decision not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### EU AI Act Compliance\n",
    "\n",
    "**Article 13 (Transparency)**:\n",
    "- ✓ Users informed when AI is used\n",
    "- ✓ Explanations provided for decisions\n",
    "- ✓ Confidence scores disclosed\n",
    "\n",
    "**Article 15 (Logging)**:\n",
    "- ✓ All decisions automatically logged\n",
    "- ✓ Unique IDs enable traceability\n",
    "- ✓ Immutable audit trail\n",
    "- ✓ 7-year retention ready\n",
    "\n",
    "**Article 17 (Quality Management)**:\n",
    "- ✓ Risk events tracked\n",
    "- ✓ Incident reporting system\n",
    "- ✓ Performance monitoring\n",
    "\n",
    "### GDPR Compatibility\n",
    "\n",
    "- Privacy-preserving hashing\n",
    "- Sensitive data encrypted\n",
    "- Audit trail without PII exposure\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Before launch**:\n",
    "- Legal review of compliance implementation\n",
    "- Encryption key management\n",
    "- Backup and retention systems\n",
    "- Regulator access procedures\n",
    "\n",
    "**After launch**:\n",
    "- Monitor log storage growth\n",
    "- Regular compliance audits\n",
    "- Incident response testing\n",
    "- Documentation updates\n",
    "\n",
    "### Remember\n",
    "\n",
    "Compliance is not optional. Build it into your architecture from the start, not as an afterthought."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
