{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 11: Hybrid Architecture Design II\n",
    "## Integration Patterns\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_11_hybrid_architecture_ii/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Implement LLM + retrieval (RAG) systems\n",
    "2. Build LLM + constraint propagation hybrids\n",
    "3. Create LLM + verification modules\n",
    "4. Test and validate hybrid systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: RAG Implementation Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"Simple RAG implementation for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, docs: List[Dict]):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        self.documents.extend(docs)\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        scored = []\n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc[\"content\"].lower().split())\n",
    "            score = len(query_words & doc_words)\n",
    "            scored.append((score, doc))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, doc in scored[:top_k]]\n",
    "    \n",
    "    def generate(self, query: str) -> Dict:\n",
    "        \"\"\"RAG pipeline: retrieve then generate.\"\"\"\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved = self.retrieve(query)\n",
    "        context = \"\\n\".join([f\"- {d['content']}\" for d in retrieved])\n",
    "        \n",
    "        # Step 2: Generate with context\n",
    "        prompt = f\"\"\"Answer based ONLY on the provided context. \n",
    "If the context doesn't contain the answer, say \"Information not available.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=300,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": response.content[0].text,\n",
    "            \"sources\": [d[\"id\"] for d in retrieved]\n",
    "        }\n",
    "\n",
    "\n",
    "# Test RAG system\n",
    "rag = SimpleRAG(client)\n",
    "rag.add_documents([\n",
    "    {\"id\": \"1\", \"content\": \"Aspirin should not be taken within 4 hours of ibuprofen due to NSAID interactions.\"},\n",
    "    {\"id\": \"2\", \"content\": \"Acetaminophen (Tylenol) is safe to take with NSAIDs.\"},\n",
    "    {\"id\": \"3\", \"content\": \"Blood pressure medications should be taken at consistent times daily.\"},\n",
    "])\n",
    "\n",
    "result = rag.generate(\"Can I take aspirin and ibuprofen together?\")\n",
    "print(f\"Question: {result['query']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Sources: {result['sources']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Constraint Solver Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduleConstraintSolver:\n",
    "    \"\"\"Simple constraint solver for scheduling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, name: str, start: int, end: int):\n",
    "        \"\"\"Add an event (times in minutes from midnight).\"\"\"\n",
    "        self.events.append({\"name\": name, \"start\": start, \"end\": end})\n",
    "    \n",
    "    def check_conflicts(self) -> List[str]:\n",
    "        \"\"\"Check for scheduling conflicts.\"\"\"\n",
    "        conflicts = []\n",
    "        for i, e1 in enumerate(self.events):\n",
    "            for e2 in self.events[i+1:]:\n",
    "                if self._overlaps(e1, e2):\n",
    "                    conflicts.append(f\"{e1['name']} conflicts with {e2['name']}\")\n",
    "        return conflicts\n",
    "    \n",
    "    def _overlaps(self, e1: Dict, e2: Dict) -> bool:\n",
    "        return e1[\"start\"] < e2[\"end\"] and e2[\"start\"] < e1[\"end\"]\n",
    "    \n",
    "    def find_slot(self, duration: int, earliest: int = 480, latest: int = 1080) -> Optional[int]:\n",
    "        \"\"\"Find available slot of given duration.\"\"\"\n",
    "        sorted_events = sorted(self.events, key=lambda x: x[\"start\"])\n",
    "        \n",
    "        # Check before first event\n",
    "        if not sorted_events or sorted_events[0][\"start\"] >= earliest + duration:\n",
    "            return earliest\n",
    "        \n",
    "        # Check between events\n",
    "        for i in range(len(sorted_events) - 1):\n",
    "            gap_start = sorted_events[i][\"end\"]\n",
    "            gap_end = sorted_events[i+1][\"start\"]\n",
    "            if gap_end - gap_start >= duration:\n",
    "                return gap_start\n",
    "        \n",
    "        # Check after last event\n",
    "        if sorted_events[-1][\"end\"] + duration <= latest:\n",
    "            return sorted_events[-1][\"end\"]\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class HybridScheduler:\n",
    "    \"\"\"Hybrid LLM + Constraint Solver scheduler.\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.solver = ScheduleConstraintSolver()\n",
    "    \n",
    "    def extract_schedule_request(self, text: str) -> Dict:\n",
    "        \"\"\"Use LLM to extract scheduling info.\"\"\"\n",
    "        prompt = f\"\"\"Extract the meeting request details from this text.\n",
    "Return JSON with: {{\"title\": string, \"duration_minutes\": number}}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=100,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        text_response = response.content[0].text\n",
    "        start = text_response.find('{')\n",
    "        end = text_response.rfind('}') + 1\n",
    "        if start >= 0 and end > start:\n",
    "            return json.loads(text_response[start:end])\n",
    "        return None\n",
    "    \n",
    "    def schedule(self, request: str) -> Dict:\n",
    "        \"\"\"Full hybrid scheduling pipeline.\"\"\"\n",
    "        # Step 1: LLM extracts info\n",
    "        extracted = self.extract_schedule_request(request)\n",
    "        if not extracted:\n",
    "            return {\"success\": False, \"error\": \"Could not extract request\"}\n",
    "        \n",
    "        # Step 2: Solver finds slot\n",
    "        slot = self.solver.find_slot(extracted[\"duration_minutes\"])\n",
    "        if slot is None:\n",
    "            return {\"success\": False, \"error\": \"No available slot\"}\n",
    "        \n",
    "        # Step 3: Add to schedule\n",
    "        self.solver.add_event(\n",
    "            extracted[\"title\"],\n",
    "            slot,\n",
    "            slot + extracted[\"duration_minutes\"]\n",
    "        )\n",
    "        \n",
    "        hours, mins = divmod(slot, 60)\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"title\": extracted[\"title\"],\n",
    "            \"time\": f\"{hours:02d}:{mins:02d}\",\n",
    "            \"duration\": extracted[\"duration_minutes\"]\n",
    "        }\n",
    "\n",
    "\n",
    "# Test hybrid scheduler\n",
    "scheduler = HybridScheduler(client)\n",
    "scheduler.solver.add_event(\"Existing Meeting\", 600, 660)  # 10:00-11:00\n",
    "\n",
    "result = scheduler.schedule(\"Schedule a 30-minute team standup\")\n",
    "print(f\"Scheduling result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 3: Testing Hybrid vs LLM-Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(scenarios: List[Dict], client) -> pd.DataFrame:\n",
    "    \"\"\"Compare hybrid vs LLM-only approaches.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # LLM-only approach\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=50,\n",
    "            messages=[{\"role\": \"user\", \"content\": scenario[\"prompt\"]}]\n",
    "        )\n",
    "        llm_answer = response.content[0].text.strip().upper()\n",
    "        llm_correct = scenario[\"expected\"] in llm_answer[:10]\n",
    "        \n",
    "        results.append({\n",
    "            \"scenario\": scenario[\"name\"],\n",
    "            \"expected\": scenario[\"expected\"],\n",
    "            \"llm_only\": \"CORRECT\" if llm_correct else \"WRONG\",\n",
    "            \"hybrid\": \"CORRECT\"  # Assume hybrid uses deterministic checker\n",
    "        })\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    {\"name\": \"Safe timing\", \"prompt\": \"Med A at 8AM, Med B at 1PM, 4hr gap needed. Safe? YES/NO\", \"expected\": \"YES\"},\n",
    "    {\"name\": \"Unsafe timing\", \"prompt\": \"Med A at 8AM, Med B at 11AM, 4hr gap needed. Safe? YES/NO\", \"expected\": \"NO\"},\n",
    "]\n",
    "\n",
    "comparison = compare_approaches(test_scenarios, client)\n",
    "print(\"\\nApproach Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RAG grounds responses in facts.** Reduces hallucination on knowledge tasks.\n",
    "\n",
    "2. **Constraint solvers guarantee correctness.** Use for scheduling, timing, resource allocation.\n",
    "\n",
    "3. **LLM handles NLU, symbolic handles logic.** Clear division of labor.\n",
    "\n",
    "4. **Always test hybrid vs baseline.** Verify improvement.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Session:** Risk Documentation and Production Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
