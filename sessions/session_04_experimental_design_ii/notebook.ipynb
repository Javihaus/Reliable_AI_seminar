{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 4: Experimental Design II\n",
    "## Statistical Analysis and Causal Testing\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_04_experimental_design_ii/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Detect bimodal distributions in model performance\n",
    "2. Quantify prompt brittleness with statistical significance\n",
    "3. Measure false positive/negative bias rates\n",
    "4. Understand causal intervention approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn scipy statsmodels\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Detecting Bimodal Distributions\n",
    "\n",
    "Model performance on specific tasks often shows bimodal rather than normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated model performance data (based on real experiments)\n",
    "# Each value is accuracy (%) for a model on temporal reasoning tasks\n",
    "\n",
    "model_accuracies = {\n",
    "    \"Phi-2 (2.7B)\": 62.5,\n",
    "    \"Phi-3 (3.8B)\": 62.5,\n",
    "    \"Mistral (7B)\": 62.5,\n",
    "    \"Llama-2 (7B)\": 37.5,\n",
    "    \"Gemma (7B)\": 50.0,\n",
    "    \"Llama-3 (8B)\": 75.0,\n",
    "    \"Qwen (7B)\": 37.5,\n",
    "    \"DeepSeek (7B)\": 62.5,\n",
    "}\n",
    "\n",
    "accuracies = list(model_accuracies.values())\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(accuracies, bins=6, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(accuracies), color='red', linestyle='--', label=f'Mean: {np.mean(accuracies):.1f}%')\n",
    "axes[0].set_xlabel('Accuracy (%)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Model Accuracies\\non Temporal Reasoning')\n",
    "axes[0].legend()\n",
    "\n",
    "# Bar chart by model\n",
    "models = list(model_accuracies.keys())\n",
    "accs = list(model_accuracies.values())\n",
    "colors = ['green' if a >= 60 else 'orange' if a >= 45 else 'red' for a in accs]\n",
    "axes[1].barh(models, accs, color=colors, alpha=0.7)\n",
    "axes[1].axvline(50, color='gray', linestyle='--', label='Random chance')\n",
    "axes[1].set_xlabel('Accuracy (%)')\n",
    "axes[1].set_title('Performance by Model')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDistribution Statistics:\")\n",
    "print(f\"  Mean: {np.mean(accuracies):.1f}%\")\n",
    "print(f\"  Std: {np.std(accuracies):.1f}%\")\n",
    "print(f\"  Min: {np.min(accuracies):.1f}%\")\n",
    "print(f\"  Max: {np.max(accuracies):.1f}%\")\n",
    "print(f\"\\nNote: Bimodal clustering around ~37.5% and ~62.5%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bimodality(data: List[float]) -> Dict:\n",
    "    \"\"\"Test for bimodality using dip test and coefficient.\"\"\"\n",
    "    \n",
    "    # Bimodality coefficient (Sarle's)\n",
    "    n = len(data)\n",
    "    skewness = stats.skew(data)\n",
    "    kurtosis = stats.kurtosis(data)\n",
    "    \n",
    "    # Bimodality coefficient formula\n",
    "    bc = (skewness**2 + 1) / (kurtosis + 3 * ((n-1)**2) / ((n-2)*(n-3)))\n",
    "    \n",
    "    # BC > 0.555 suggests bimodality\n",
    "    is_bimodal = bc > 0.555\n",
    "    \n",
    "    return {\n",
    "        \"bimodality_coefficient\": bc,\n",
    "        \"is_bimodal\": is_bimodal,\n",
    "        \"skewness\": skewness,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"interpretation\": \"Bimodal distribution detected\" if is_bimodal else \"Unimodal distribution\"\n",
    "    }\n",
    "\n",
    "bimodality_result = test_bimodality(accuracies)\n",
    "print(\"Bimodality Test Results:\")\n",
    "for k, v in bimodality_result.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Quantifying Brittleness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated brittleness data: accuracy by prompt format\n",
    "brittleness_data = {\n",
    "    \"natural\": [75, 62.5, 75, 62.5, 50],\n",
    "    \"clinical\": [50, 37.5, 50, 50, 25],\n",
    "    \"json\": [25, 12.5, 25, 25, 0],\n",
    "    \"conversational\": [62.5, 50, 62.5, 50, 37.5],\n",
    "    \"formal\": [62.5, 50, 75, 62.5, 50]\n",
    "}\n",
    "\n",
    "df_brittleness = pd.DataFrame(brittleness_data)\n",
    "df_brittleness.index = [f\"Model {i+1}\" for i in range(len(df_brittleness))]\n",
    "\n",
    "# Calculate brittleness metrics\n",
    "df_brittleness['max_accuracy'] = df_brittleness.max(axis=1)\n",
    "df_brittleness['min_accuracy'] = df_brittleness.min(axis=1)\n",
    "df_brittleness['brittleness_pp'] = df_brittleness['max_accuracy'] - df_brittleness['min_accuracy']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRITTLENESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(df_brittleness.to_string())\n",
    "\n",
    "print(f\"\\nMean brittleness: {df_brittleness['brittleness_pp'].mean():.1f} percentage points\")\n",
    "print(f\"Max brittleness: {df_brittleness['brittleness_pp'].max():.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize brittleness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of accuracies by format\n",
    "format_cols = ['natural', 'clinical', 'json', 'conversational', 'formal']\n",
    "sns.heatmap(\n",
    "    df_brittleness[format_cols], \n",
    "    annot=True, \n",
    "    fmt='.1f',\n",
    "    cmap='RdYlGn',\n",
    "    ax=axes[0],\n",
    "    vmin=0,\n",
    "    vmax=100\n",
    ")\n",
    "axes[0].set_title('Accuracy by Prompt Format (%)')\n",
    "\n",
    "# Brittleness by model\n",
    "axes[1].barh(\n",
    "    df_brittleness.index, \n",
    "    df_brittleness['brittleness_pp'],\n",
    "    color=['red' if b > 50 else 'orange' if b > 30 else 'green' for b in df_brittleness['brittleness_pp']]\n",
    ")\n",
    "axes[1].axvline(30, color='gray', linestyle='--', label='High threshold')\n",
    "axes[1].set_xlabel('Brittleness (percentage points)')\n",
    "axes[1].set_title('Brittleness by Model')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 3: Measuring Bias Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias_rates(predictions: List[str], ground_truth: List[str]) -> Dict:\n",
    "    \"\"\"Calculate false positive and false negative rates.\"\"\"\n",
    "    \n",
    "    # Count outcomes\n",
    "    tp = sum(1 for p, g in zip(predictions, ground_truth) if p == \"YES\" and g == \"YES\")\n",
    "    tn = sum(1 for p, g in zip(predictions, ground_truth) if p == \"NO\" and g == \"NO\")\n",
    "    fp = sum(1 for p, g in zip(predictions, ground_truth) if p == \"YES\" and g == \"NO\")\n",
    "    fn = sum(1 for p, g in zip(predictions, ground_truth) if p == \"NO\" and g == \"YES\")\n",
    "    \n",
    "    # Calculate rates\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    accuracy = (tp + tn) / len(predictions)\n",
    "    \n",
    "    return {\n",
    "        \"true_positives\": tp,\n",
    "        \"true_negatives\": tn,\n",
    "        \"false_positives\": fp,\n",
    "        \"false_negatives\": fn,\n",
    "        \"false_positive_rate\": fpr,\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Action bias demonstration\n",
    "# Model that always recommends action (says YES)\n",
    "ground_truth = [\"YES\", \"YES\", \"YES\", \"YES\", \"NO\", \"NO\", \"NO\", \"NO\"]\n",
    "biased_predictions = [\"YES\", \"YES\", \"YES\", \"YES\", \"YES\", \"YES\", \"YES\", \"YES\"]  # Always YES\n",
    "unbiased_predictions = [\"YES\", \"YES\", \"NO\", \"YES\", \"NO\", \"NO\", \"YES\", \"NO\"]  # Mixed\n",
    "\n",
    "biased_results = calculate_bias_rates(biased_predictions, ground_truth)\n",
    "unbiased_results = calculate_bias_rates(unbiased_predictions, ground_truth)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIAS RATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nBiased Model (always says YES):\")\n",
    "print(f\"  Accuracy: {biased_results['accuracy']:.1%}\")\n",
    "print(f\"  False Positive Rate: {biased_results['false_positive_rate']:.1%}\")\n",
    "print(f\"  False Negative Rate: {biased_results['false_negative_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nUnbiased Model (mixed responses):\")\n",
    "print(f\"  Accuracy: {unbiased_results['accuracy']:.1%}\")\n",
    "print(f\"  False Positive Rate: {unbiased_results['false_positive_rate']:.1%}\")\n",
    "print(f\"  False Negative Rate: {unbiased_results['false_negative_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Insight: 100% FPR means the model never correctly identifies\")\n",
    "print(\"'unsafe' scenarios - critical for high-stakes applications!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "def plot_confusion_matrix(results: Dict, title: str):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = np.array([\n",
    "        [results['true_negatives'], results['false_positives']],\n",
    "        [results['false_negatives'], results['true_positives']]\n",
    "    ])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Predicted NO', 'Predicted YES'],\n",
    "        yticklabels=['Actual NO', 'Actual YES'],\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{title}\\nAccuracy: {results[\"accuracy\"]:.1%}, FPR: {results[\"false_positive_rate\"]:.1%}')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_confusion_matrix(biased_results, \"Biased Model (Always YES)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 4: Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_format_difference(format_a_results: List[bool], format_b_results: List[bool]) -> Dict:\n",
    "    \"\"\"Test if performance difference between formats is significant.\"\"\"\n",
    "    \n",
    "    # McNemar's test for paired nominal data\n",
    "    # Count disagreements\n",
    "    b = sum(1 for a, b in zip(format_a_results, format_b_results) if a and not b)  # A correct, B wrong\n",
    "    c = sum(1 for a, b in zip(format_a_results, format_b_results) if not a and b)  # A wrong, B correct\n",
    "    \n",
    "    # McNemar's statistic\n",
    "    if b + c > 0:\n",
    "        statistic = (abs(b - c) - 1)**2 / (b + c)\n",
    "        p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    else:\n",
    "        statistic = 0\n",
    "        p_value = 1.0\n",
    "    \n",
    "    accuracy_a = sum(format_a_results) / len(format_a_results)\n",
    "    accuracy_b = sum(format_b_results) / len(format_b_results)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_format_a\": accuracy_a,\n",
    "        \"accuracy_format_b\": accuracy_b,\n",
    "        \"difference_pp\": (accuracy_a - accuracy_b) * 100,\n",
    "        \"mcnemar_statistic\": statistic,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < 0.05\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Compare natural vs JSON format\n",
    "# Simulated paired results\n",
    "natural_results = [True, True, False, True, True, True, False, True]  # 75% accuracy\n",
    "json_results = [False, True, False, False, False, True, False, False]  # 25% accuracy\n",
    "\n",
    "significance = test_format_difference(natural_results, json_results)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE: Natural vs JSON Format\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Natural format accuracy: {significance['accuracy_format_a']:.1%}\")\n",
    "print(f\"JSON format accuracy: {significance['accuracy_format_b']:.1%}\")\n",
    "print(f\"Difference: {significance['difference_pp']:.1f} percentage points\")\n",
    "print(f\"McNemar statistic: {significance['mcnemar_statistic']:.2f}\")\n",
    "print(f\"P-value: {significance['p_value']:.4f}\")\n",
    "print(f\"Significant (p<0.05): {'YES' if significance['significant'] else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 5: Exercise - Analyze Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXERCISE: Analyze your test results\n",
    "\n",
    "# Replace with your actual data\n",
    "my_predictions = [\"YES\", \"NO\", \"YES\", \"YES\", \"NO\", \"NO\", \"YES\", \"NO\"]\n",
    "my_ground_truth = [\"YES\", \"YES\", \"NO\", \"YES\", \"NO\", \"NO\", \"NO\", \"NO\"]\n",
    "\n",
    "my_results = calculate_bias_rates(my_predictions, my_ground_truth)\n",
    "\n",
    "print(\"YOUR RESULTS:\")\n",
    "print(f\"  Accuracy: {my_results['accuracy']:.1%}\")\n",
    "print(f\"  False Positive Rate: {my_results['false_positive_rate']:.1%}\")\n",
    "print(f\"  False Negative Rate: {my_results['false_negative_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Bimodal distributions are diagnostic.** Models either \"get it\" or don't - no continuous scaling.\n",
    "\n",
    "2. **Brittleness reveals pattern matching.** >30pp accuracy change across formats = not robust understanding.\n",
    "\n",
    "3. **Bias rates matter more than accuracy.** A 100% FPR is catastrophic even with 50% overall accuracy.\n",
    "\n",
    "4. **Test for significance.** Small sample differences may not be meaningful.\n",
    "\n",
    "5. **Visualize everything.** Patterns become obvious in good visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "**Homework:** Complete statistical analysis of your test results.\n",
    "\n",
    "**Next Session:** Failure Mode Iâ€”Temporal Constraint Processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
