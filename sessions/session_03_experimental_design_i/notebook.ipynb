{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 3: Experimental Design I\n",
    "## Constructing Diagnostic Scenarios\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_03_experimental_design_i/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Design balanced test distributions\n",
    "2. Control semantic content while varying format\n",
    "3. Establish deterministic ground truth\n",
    "4. Create scenario sets for your deployment domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Why Balanced Test Sets Matter\n",
    "\n",
    "Unbalanced test sets can hide severe failures behind high accuracy numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_balance_importance():\n",
    "    \"\"\"Show how unbalanced tests hide failures.\"\"\"\n",
    "    \n",
    "    # Simulate a model that always says \"YES\"\n",
    "    always_yes_model = lambda x: \"YES\"\n",
    "    \n",
    "    # Unbalanced test set (90% positive)\n",
    "    unbalanced_tests = [\n",
    "        {\"scenario\": f\"Positive case {i}\", \"correct\": \"YES\"} for i in range(9)\n",
    "    ] + [\n",
    "        {\"scenario\": \"Negative case\", \"correct\": \"NO\"}\n",
    "    ]\n",
    "    \n",
    "    # Balanced test set (50% positive)\n",
    "    balanced_tests = [\n",
    "        {\"scenario\": f\"Positive case {i}\", \"correct\": \"YES\"} for i in range(5)\n",
    "    ] + [\n",
    "        {\"scenario\": f\"Negative case {i}\", \"correct\": \"NO\"} for i in range(5)\n",
    "    ]\n",
    "    \n",
    "    # Calculate accuracy for both\n",
    "    unbalanced_correct = sum(1 for t in unbalanced_tests if always_yes_model(t) == t[\"correct\"])\n",
    "    balanced_correct = sum(1 for t in balanced_tests if always_yes_model(t) == t[\"correct\"])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL THAT ALWAYS SAYS 'YES'\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nUnbalanced test set (90% positive):\")\n",
    "    print(f\"  Accuracy: {unbalanced_correct}/{len(unbalanced_tests)} = {unbalanced_correct/len(unbalanced_tests):.1%}\")\n",
    "    print(f\"  Looks great! But...\")\n",
    "    \n",
    "    print(f\"\\nBalanced test set (50% positive):\")\n",
    "    print(f\"  Accuracy: {balanced_correct}/{len(balanced_tests)} = {balanced_correct/len(balanced_tests):.1%}\")\n",
    "    print(f\"  Reveals the true failure!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LESSON: Always use balanced test sets to reveal true performance.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "demonstrate_balance_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 2: Test Scenario Design Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestScenario:\n",
    "    \"\"\"A single test scenario with ground truth.\"\"\"\n",
    "    id: str\n",
    "    description: str\n",
    "    correct_answer: str\n",
    "    category: str  # e.g., \"positive\", \"negative\", \"edge_case\"\n",
    "    difficulty: str  # e.g., \"easy\", \"medium\", \"hard\"\n",
    "    formats: Dict[str, str] = None  # Multiple prompt formats\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.formats is None:\n",
    "            self.formats = {}\n",
    "\n",
    "\n",
    "class TestSuiteBuilder:\n",
    "    \"\"\"Build balanced, multi-format test suites.\"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str):\n",
    "        self.domain = domain\n",
    "        self.scenarios = []\n",
    "    \n",
    "    def add_scenario(self, scenario: TestScenario):\n",
    "        \"\"\"Add a test scenario.\"\"\"\n",
    "        self.scenarios.append(scenario)\n",
    "    \n",
    "    def check_balance(self) -> Dict:\n",
    "        \"\"\"Check distribution balance.\"\"\"\n",
    "        categories = {}\n",
    "        for s in self.scenarios:\n",
    "            categories[s.category] = categories.get(s.category, 0) + 1\n",
    "        \n",
    "        total = len(self.scenarios)\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"distribution\": {k: f\"{v}/{total} ({v/total:.1%})\" for k, v in categories.items()},\n",
    "            \"is_balanced\": max(categories.values()) - min(categories.values()) <= 1 if categories else True\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate test suite report.\"\"\"\n",
    "        balance = self.check_balance()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"TEST SUITE: {self.domain}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nTotal scenarios: {balance['total']}\")\n",
    "        print(f\"\\nDistribution:\")\n",
    "        for cat, dist in balance['distribution'].items():\n",
    "            print(f\"  {cat}: {dist}\")\n",
    "        print(f\"\\nBalanced: {'YES' if balance['is_balanced'] else 'NO - NEEDS ADJUSTMENT'}\")\n",
    "        \n",
    "        # Format coverage\n",
    "        format_counts = {}\n",
    "        for s in self.scenarios:\n",
    "            for fmt in s.formats.keys():\n",
    "                format_counts[fmt] = format_counts.get(fmt, 0) + 1\n",
    "        \n",
    "        if format_counts:\n",
    "            print(f\"\\nFormat coverage:\")\n",
    "            for fmt, count in format_counts.items():\n",
    "                print(f\"  {fmt}: {count}/{balance['total']} scenarios\")\n",
    "\n",
    "\n",
    "# Example: Build a medication timing test suite\n",
    "builder = TestSuiteBuilder(\"Medication Timing Constraints\")\n",
    "\n",
    "# Add positive cases (safe to take medication)\n",
    "builder.add_scenario(TestScenario(\n",
    "    id=\"MED001\",\n",
    "    description=\"Aspirin at 8AM, ibuprofen at 1PM, 4-hour requirement\",\n",
    "    correct_answer=\"YES\",\n",
    "    category=\"positive\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"I took aspirin at 8:00 AM. Can I take ibuprofen at 1:00 PM? (4-hour wait required) YES or NO?\",\n",
    "        \"clinical\": \"Med A: 0800h. Med B requested: 1300h. Interval: 4h. Safe? YES/NO\",\n",
    "        \"json\": '{\"med_a\": \"8:00 AM\", \"med_b\": \"1:00 PM\", \"min_gap\": 4} Safe? YES/NO'\n",
    "    }\n",
    "))\n",
    "\n",
    "builder.add_scenario(TestScenario(\n",
    "    id=\"MED002\",\n",
    "    description=\"Morning dose at 6AM, afternoon at 2PM, 6-hour requirement\",\n",
    "    correct_answer=\"YES\",\n",
    "    category=\"positive\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"Took morning dose at 6:00 AM. Is 2:00 PM safe for afternoon dose? (6-hour minimum) YES or NO?\",\n",
    "        \"clinical\": \"Dose 1: 0600h. Dose 2 proposed: 1400h. Required gap: 6h. Compliant? YES/NO\"\n",
    "    }\n",
    "))\n",
    "\n",
    "# Add negative cases (NOT safe)\n",
    "builder.add_scenario(TestScenario(\n",
    "    id=\"MED003\",\n",
    "    description=\"Aspirin at 8AM, ibuprofen at 11AM, 4-hour requirement\",\n",
    "    correct_answer=\"NO\",\n",
    "    category=\"negative\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"I took aspirin at 8:00 AM. Can I take ibuprofen at 11:00 AM? (4-hour wait required) YES or NO?\",\n",
    "        \"clinical\": \"Med A: 0800h. Med B requested: 1100h. Interval: 4h. Safe? YES/NO\",\n",
    "        \"json\": '{\"med_a\": \"8:00 AM\", \"med_b\": \"11:00 AM\", \"min_gap\": 4} Safe? YES/NO'\n",
    "    }\n",
    "))\n",
    "\n",
    "builder.add_scenario(TestScenario(\n",
    "    id=\"MED004\",\n",
    "    description=\"Morning dose at 7AM, noon at 12PM, 6-hour requirement\",\n",
    "    correct_answer=\"NO\",\n",
    "    category=\"negative\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"Took morning dose at 7:00 AM. Is 12:00 PM safe? (6-hour minimum) YES or NO?\",\n",
    "        \"clinical\": \"Dose 1: 0700h. Dose 2 proposed: 1200h. Required gap: 6h. Compliant? YES/NO\"\n",
    "    }\n",
    "))\n",
    "\n",
    "builder.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 3: Running Multi-Format Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiformat_test(scenario: TestScenario, client) -> Dict:\n",
    "    \"\"\"Test a scenario across all its formats.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"scenario_id\": scenario.id,\n",
    "        \"correct_answer\": scenario.correct_answer,\n",
    "        \"format_results\": {}\n",
    "    }\n",
    "    \n",
    "    for format_name, prompt in scenario.formats.items():\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text.strip().upper()\n",
    "        if \"YES\" in answer[:5]:\n",
    "            answer = \"YES\"\n",
    "        elif \"NO\" in answer[:5]:\n",
    "            answer = \"NO\"\n",
    "        else:\n",
    "            answer = \"UNCLEAR\"\n",
    "        \n",
    "        correct = answer == scenario.correct_answer\n",
    "        \n",
    "        results[\"format_results\"][format_name] = {\n",
    "            \"answer\": answer,\n",
    "            \"correct\": correct\n",
    "        }\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # Calculate consistency\n",
    "    answers = [r[\"answer\"] for r in results[\"format_results\"].values()]\n",
    "    results[\"consistent\"] = len(set(answers)) == 1\n",
    "    results[\"accuracy\"] = sum(r[\"correct\"] for r in results[\"format_results\"].values()) / len(results[\"format_results\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run tests on our scenarios\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-FORMAT TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = []\n",
    "for scenario in builder.scenarios:\n",
    "    result = run_multiformat_test(scenario, client)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\n{scenario.id}: Expected {scenario.correct_answer}\")\n",
    "    for fmt, res in result[\"format_results\"].items():\n",
    "        status = \"CORRECT\" if res[\"correct\"] else \"WRONG\"\n",
    "        print(f\"  {fmt}: {res['answer']} - {status}\")\n",
    "    print(f\"  Consistency: {'YES' if result['consistent'] else 'NO (BRITTLE)'}\")\n",
    "    print(f\"  Accuracy: {result['accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 4: Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_test_results(results: List[Dict], scenarios: List[TestScenario]):\n",
    "    \"\"\"Analyze test results for key metrics.\"\"\"\n",
    "    \n",
    "    # Overall accuracy\n",
    "    total_tests = sum(len(r[\"format_results\"]) for r in results)\n",
    "    total_correct = sum(\n",
    "        sum(fr[\"correct\"] for fr in r[\"format_results\"].values())\n",
    "        for r in results\n",
    "    )\n",
    "    \n",
    "    # By category\n",
    "    category_results = {}\n",
    "    for result, scenario in zip(results, scenarios):\n",
    "        cat = scenario.category\n",
    "        if cat not in category_results:\n",
    "            category_results[cat] = {\"correct\": 0, \"total\": 0}\n",
    "        \n",
    "        for fr in result[\"format_results\"].values():\n",
    "            category_results[cat][\"total\"] += 1\n",
    "            if fr[\"correct\"]:\n",
    "                category_results[cat][\"correct\"] += 1\n",
    "    \n",
    "    # By format\n",
    "    format_results = {}\n",
    "    for result in results:\n",
    "        for fmt, fr in result[\"format_results\"].items():\n",
    "            if fmt not in format_results:\n",
    "                format_results[fmt] = {\"correct\": 0, \"total\": 0}\n",
    "            format_results[fmt][\"total\"] += 1\n",
    "            if fr[\"correct\"]:\n",
    "                format_results[fmt][\"correct\"] += 1\n",
    "    \n",
    "    # Consistency\n",
    "    consistent_count = sum(1 for r in results if r[\"consistent\"])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {total_correct}/{total_tests} = {total_correct/total_tests:.1%}\")\n",
    "    \n",
    "    print(f\"\\nBy Category:\")\n",
    "    for cat, data in category_results.items():\n",
    "        acc = data[\"correct\"] / data[\"total\"] if data[\"total\"] > 0 else 0\n",
    "        print(f\"  {cat}: {data['correct']}/{data['total']} = {acc:.1%}\")\n",
    "    \n",
    "    print(f\"\\nBy Format:\")\n",
    "    format_accuracies = []\n",
    "    for fmt, data in format_results.items():\n",
    "        acc = data[\"correct\"] / data[\"total\"] if data[\"total\"] > 0 else 0\n",
    "        format_accuracies.append(acc)\n",
    "        print(f\"  {fmt}: {data['correct']}/{data['total']} = {acc:.1%}\")\n",
    "    \n",
    "    if format_accuracies:\n",
    "        brittleness = (max(format_accuracies) - min(format_accuracies)) * 100\n",
    "        print(f\"\\nBrittleness (max-min accuracy): {brittleness:.1f} percentage points\")\n",
    "    \n",
    "    print(f\"\\nConsistency: {consistent_count}/{len(results)} scenarios gave same answer across formats\")\n",
    "\n",
    "\n",
    "analyze_test_results(all_results, builder.scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 5: Exercise - Build Your Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXERCISE: Build a test suite for your domain\n",
    "\n",
    "my_builder = TestSuiteBuilder(\"YOUR DOMAIN HERE\")\n",
    "\n",
    "# Add your positive cases\n",
    "my_builder.add_scenario(TestScenario(\n",
    "    id=\"TEST001\",\n",
    "    description=\"Describe your positive test case\",\n",
    "    correct_answer=\"YES\",  # or whatever the correct answer is\n",
    "    category=\"positive\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"Your natural language prompt here\",\n",
    "        \"formal\": \"Your formal prompt here\",\n",
    "        # Add more formats\n",
    "    }\n",
    "))\n",
    "\n",
    "# Add your negative cases\n",
    "my_builder.add_scenario(TestScenario(\n",
    "    id=\"TEST002\",\n",
    "    description=\"Describe your negative test case\",\n",
    "    correct_answer=\"NO\",\n",
    "    category=\"negative\",\n",
    "    difficulty=\"easy\",\n",
    "    formats={\n",
    "        \"natural\": \"Your natural language prompt here\",\n",
    "        \"formal\": \"Your formal prompt here\",\n",
    "    }\n",
    "))\n",
    "\n",
    "# Generate report\n",
    "my_builder.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Balance your test sets.** Unbalanced distributions hide failures behind high accuracy.\n",
    "\n",
    "2. **Test multiple formats.** Same semantics, different formats reveals brittleness.\n",
    "\n",
    "3. **Establish ground truth first.** Know the correct answer before testing.\n",
    "\n",
    "4. **Measure consistency.** Inconsistent answers across formats = pattern matching.\n",
    "\n",
    "5. **Include edge cases.** Boundary conditions reveal architectural limitations.\n",
    "\n",
    "---\n",
    "\n",
    "**Homework:** Build a complete test suite with 8+ scenarios for your deployment domain.\n",
    "\n",
    "**Next Session:** Experimental Design IIâ€”Statistical Analysis and Causal Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
