{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 6: Knowledge Scaling Pathology\n",
    "## The Confidence-Competence Gap\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_06_failure_mode_scaling/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Identify tasks exhibiting scaling pathology\n",
    "2. Calculate the confidence-competence gap metric\n",
    "3. Recognize when larger models won't help\n",
    "4. Recommend appropriate model sizes by task type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn scipy\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: The Scaling Paradox\n",
    "\n",
    "Conventional wisdom: Bigger models = better performance.\n",
    "\n",
    "Reality: On certain tasks, bigger models fail more confidently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated scaling data based on real experiments\n",
    "# Task: Temporal constraint reasoning\n",
    "\n",
    "scaling_data = {\n",
    "    \"model_size_B\": [1.0, 2.7, 3.8, 7.0, 8.0, 13.0, 70.0],\n",
    "    \"loss\": [2.8, 2.4, 2.1, 1.8, 1.6, 1.4, 1.0],  # Lower = better (perplexity-like)\n",
    "    \"accuracy\": [45, 50, 62.5, 50, 75, 62.5, 65],  # Temporal task accuracy\n",
    "    \"confidence\": [60, 70, 75, 80, 85, 90, 95],  # Model's stated confidence\n",
    "}\n",
    "\n",
    "df_scaling = pd.DataFrame(scaling_data)\n",
    "\n",
    "# Visualize the paradox\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Loss vs Size (looks good!)\n",
    "axes[0].plot(df_scaling['model_size_B'], df_scaling['loss'], 'b-o', linewidth=2)\n",
    "axes[0].set_xlabel('Model Size (B parameters)')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss vs Model Size\\n(Looks great!)')\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Accuracy vs Size (not so good...)\n",
    "axes[1].plot(df_scaling['model_size_B'], df_scaling['accuracy'], 'r-o', linewidth=2)\n",
    "axes[1].axhline(50, color='gray', linestyle='--', label='Random chance')\n",
    "axes[1].set_xlabel('Model Size (B parameters)')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy vs Model Size\\n(No improvement!)')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].legend()\n",
    "\n",
    "# Confidence vs Size (problematic!)\n",
    "axes[2].plot(df_scaling['model_size_B'], df_scaling['confidence'], 'g-o', linewidth=2)\n",
    "axes[2].plot(df_scaling['model_size_B'], df_scaling['accuracy'], 'r--o', alpha=0.5, label='Actual accuracy')\n",
    "axes[2].set_xlabel('Model Size (B parameters)')\n",
    "axes[2].set_ylabel('Percentage')\n",
    "axes[2].set_title('Confidence vs Accuracy\\n(Growing gap!)')\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].legend(['Confidence', 'Accuracy'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation: Loss improves by 64% while accuracy stays flat!\")\n",
    "print(\"Confidence increases from 60% to 95% while accuracy barely changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 2: Calculating the Confidence-Competence Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ccg(sizes: List[float], losses: List[float], accuracies: List[float]) -> Dict:\n",
    "    \"\"\"Calculate the Confidence-Competence Gap.\n",
    "    \n",
    "    CCG = (∂L/∂N) / (∂A/∂N)\n",
    "    \n",
    "    A highly negative CCG indicates pathological scaling:\n",
    "    loss improves while accuracy doesn't (or gets worse).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate derivatives using linear regression in log space\n",
    "    log_sizes = np.log(sizes)\n",
    "    \n",
    "    # Loss vs size slope\n",
    "    loss_slope, loss_intercept, _, _, _ = stats.linregress(log_sizes, losses)\n",
    "    \n",
    "    # Accuracy vs size slope  \n",
    "    acc_slope, acc_intercept, _, _, _ = stats.linregress(log_sizes, accuracies)\n",
    "    \n",
    "    # CCG calculation\n",
    "    if abs(acc_slope) < 0.01:  # Near-zero accuracy improvement\n",
    "        ccg = float('-inf') if loss_slope < 0 else float('inf')\n",
    "        interpretation = \"Pathological: Loss improves but accuracy flat\"\n",
    "    else:\n",
    "        ccg = loss_slope / acc_slope\n",
    "        if ccg < -10:\n",
    "            interpretation = \"Severe pathology: Confident failures\"\n",
    "        elif ccg < 0:\n",
    "            interpretation = \"Mild pathology: Some scaling issues\"\n",
    "        else:\n",
    "            interpretation = \"Healthy: Scaling helps\"\n",
    "    \n",
    "    return {\n",
    "        \"ccg\": ccg,\n",
    "        \"loss_slope\": loss_slope,\n",
    "        \"accuracy_slope\": acc_slope,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"loss_improvement_pct\": (losses[0] - losses[-1]) / losses[0] * 100,\n",
    "        \"accuracy_change_pp\": accuracies[-1] - accuracies[0]\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate CCG for temporal task\n",
    "temporal_ccg = calculate_ccg(\n",
    "    df_scaling['model_size_B'].tolist(),\n",
    "    df_scaling['loss'].tolist(),\n",
    "    df_scaling['accuracy'].tolist()\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIDENCE-COMPETENCE GAP ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTask: Temporal Constraint Reasoning\")\n",
    "print(f\"\\nCCG Value: {temporal_ccg['ccg']:.2f}\")\n",
    "print(f\"Loss slope: {temporal_ccg['loss_slope']:.4f} (negative = improving)\")\n",
    "print(f\"Accuracy slope: {temporal_ccg['accuracy_slope']:.4f} (positive = improving)\")\n",
    "print(f\"\\nLoss improved: {temporal_ccg['loss_improvement_pct']:.1f}%\")\n",
    "print(f\"Accuracy changed: {temporal_ccg['accuracy_change_pp']:.1f} percentage points\")\n",
    "print(f\"\\nInterpretation: {temporal_ccg['interpretation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Task Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scaling behavior across task types\n",
    "\n",
    "task_scaling = {\n",
    "    \"Pattern Completion\": {\n",
    "        \"sizes\": [1, 3, 7, 13, 70],\n",
    "        \"losses\": [3.0, 2.5, 2.0, 1.5, 1.0],\n",
    "        \"accuracies\": [60, 70, 80, 88, 95]\n",
    "    },\n",
    "    \"Knowledge Retrieval\": {\n",
    "        \"sizes\": [1, 3, 7, 13, 70],\n",
    "        \"losses\": [2.8, 2.3, 1.9, 1.5, 1.1],\n",
    "        \"accuracies\": [40, 55, 65, 72, 80]\n",
    "    },\n",
    "    \"Temporal Reasoning\": {\n",
    "        \"sizes\": [1, 3, 7, 13, 70],\n",
    "        \"losses\": [2.8, 2.4, 2.0, 1.5, 1.0],\n",
    "        \"accuracies\": [45, 50, 55, 52, 55]\n",
    "    },\n",
    "    \"Compositional\": {\n",
    "        \"sizes\": [1, 3, 7, 13, 70],\n",
    "        \"losses\": [2.9, 2.5, 2.1, 1.6, 1.1],\n",
    "        \"accuracies\": [30, 35, 40, 38, 42]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate CCG for each task\n",
    "results = []\n",
    "for task_name, data in task_scaling.items():\n",
    "    ccg_result = calculate_ccg(data[\"sizes\"], data[\"losses\"], data[\"accuracies\"])\n",
    "    results.append({\n",
    "        \"task\": task_name,\n",
    "        \"ccg\": ccg_result[\"ccg\"] if ccg_result[\"ccg\"] != float('-inf') else -100,\n",
    "        \"loss_improvement\": ccg_result[\"loss_improvement_pct\"],\n",
    "        \"accuracy_change\": ccg_result[\"accuracy_change_pp\"],\n",
    "        \"interpretation\": ccg_result[\"interpretation\"]\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCALING BEHAVIOR BY TASK TYPE\")\n",
    "print(\"=\" * 70)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling by task type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "colors = ['green', 'blue', 'red', 'orange']\n",
    "tasks = list(task_scaling.keys())\n",
    "\n",
    "for i, (task_name, data) in enumerate(task_scaling.items()):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax.plot(data[\"sizes\"], data[\"accuracies\"], f'{colors[i][0]}-o', linewidth=2, markersize=8)\n",
    "    ax.axhline(50, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Model Size (B parameters)')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'{task_name}\\nAccuracy vs Scale')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    # Annotate with CCG\n",
    "    ccg_val = df_results[df_results['task'] == task_name]['ccg'].values[0]\n",
    "    ax.annotate(f'CCG: {ccg_val:.1f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 4: Model Size Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_model_size(task_type: str, ccg: float, accuracy_change: float) -> Dict:\n",
    "    \"\"\"Recommend model size strategy based on scaling analysis.\"\"\"\n",
    "    \n",
    "    if accuracy_change > 20 and ccg > -5:\n",
    "        return {\n",
    "            \"recommendation\": \"Scale up\",\n",
    "            \"reasoning\": \"Scaling provides meaningful accuracy improvements\",\n",
    "            \"suggested_size\": \"Largest available within budget\",\n",
    "            \"hybrid_needed\": False\n",
    "        }\n",
    "    elif accuracy_change > 10 and ccg > -20:\n",
    "        return {\n",
    "            \"recommendation\": \"Moderate scaling\",\n",
    "            \"reasoning\": \"Some benefit from scaling, diminishing returns\",\n",
    "            \"suggested_size\": \"7-13B range\",\n",
    "            \"hybrid_needed\": False\n",
    "        }\n",
    "    elif accuracy_change < 10 or ccg < -20:\n",
    "        return {\n",
    "            \"recommendation\": \"Don't scale - use hybrid\",\n",
    "            \"reasoning\": \"Scaling doesn't help; architectural limitation\",\n",
    "            \"suggested_size\": \"Smallest that handles NLU (3-7B)\",\n",
    "            \"hybrid_needed\": True\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"recommendation\": \"Test before deciding\",\n",
    "            \"reasoning\": \"Mixed scaling behavior\",\n",
    "            \"suggested_size\": \"Run experiments at multiple sizes\",\n",
    "            \"hybrid_needed\": \"Maybe\"\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL SIZE RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    rec = recommend_model_size(row['task'], row['ccg'], row['accuracy_change'])\n",
    "    print(f\"\\n{row['task']}:\")\n",
    "    print(f\"  Recommendation: {rec['recommendation']}\")\n",
    "    print(f\"  Reasoning: {rec['reasoning']}\")\n",
    "    print(f\"  Suggested size: {rec['suggested_size']}\")\n",
    "    print(f\"  Hybrid needed: {rec['hybrid_needed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 5: Exercise - Analyze Your Task's Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXERCISE: Analyze scaling for your task\n",
    "\n",
    "# Replace with your actual measurements\n",
    "my_scaling_data = {\n",
    "    \"sizes\": [1, 7, 70],  # Model sizes in billions\n",
    "    \"losses\": [2.5, 1.8, 1.2],  # Lower = better\n",
    "    \"accuracies\": [50, 55, 60]  # Your accuracy measurements\n",
    "}\n",
    "\n",
    "my_ccg = calculate_ccg(\n",
    "    my_scaling_data[\"sizes\"],\n",
    "    my_scaling_data[\"losses\"],\n",
    "    my_scaling_data[\"accuracies\"]\n",
    ")\n",
    "\n",
    "my_recommendation = recommend_model_size(\n",
    "    \"Your Task\",\n",
    "    my_ccg[\"ccg\"] if my_ccg[\"ccg\"] != float('-inf') else -100,\n",
    "    my_ccg[\"accuracy_change_pp\"]\n",
    ")\n",
    "\n",
    "print(\"YOUR SCALING ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"CCG: {my_ccg['ccg']}\")\n",
    "print(f\"Interpretation: {my_ccg['interpretation']}\")\n",
    "print(f\"\\nRecommendation: {my_recommendation['recommendation']}\")\n",
    "print(f\"Hybrid needed: {my_recommendation['hybrid_needed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Loss improvement doesn't guarantee accuracy improvement.** Models can get better at being confidently wrong.\n",
    "\n",
    "2. **CCG quantifies scaling pathology.** Highly negative CCG = scaling won't help.\n",
    "\n",
    "3. **Task type determines scaling behavior:**\n",
    "   - Pattern completion: Scales well\n",
    "   - Temporal/compositional: Pathological scaling\n",
    "\n",
    "4. **Hybrid over scaling.** When CCG is pathological, invest in hybrid architectures, not larger models.\n",
    "\n",
    "5. **Test before you scale.** Always verify scaling helps your specific task.\n",
    "\n",
    "---\n",
    "\n",
    "**Homework:** Calculate CCG for your deployment task across at least 3 model sizes.\n",
    "\n",
    "**Next Session:** Failure Mode III—Prompt Brittleness and Pattern Matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
