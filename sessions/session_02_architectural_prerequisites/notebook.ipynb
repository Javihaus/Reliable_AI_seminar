{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 2: Architectural Prerequisites\n",
    "## Why Next-Token Prediction Fails\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_02_architectural_prerequisites/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Distinguish between discrete and continuous representations\n",
    "2. Identify pattern matching vs computational processes\n",
    "3. Recognize biological existence proofs for specialized reasoning\n",
    "4. Apply Lake & Baroni's compositional generalization framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Discrete vs Continuous Representations\n",
    "\n",
    "LLMs operate on discrete tokens. This is fundamental to their architecture and determines what they can and cannot do.\n",
    "\n",
    "| Task Type | Representation | LLM Fit |\n",
    "|-----------|---------------|----------|\n",
    "| Text completion | Discrete (tokens) | Excellent |\n",
    "| Classification | Discrete (categories) | Good |\n",
    "| Arithmetic | Continuous (magnitudes) | Poor |\n",
    "| Temporal reasoning | Continuous (durations) | Very Poor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_discrete_vs_continuous():\n",
    "    \"\"\"Demonstrate LLM performance on discrete vs continuous tasks.\"\"\"\n",
    "    \n",
    "    tests = [\n",
    "        # Discrete tasks (should work well)\n",
    "        {\n",
    "            \"type\": \"discrete\",\n",
    "            \"name\": \"Category classification\",\n",
    "            \"prompt\": \"Classify this as POSITIVE or NEGATIVE: 'I love this product!'\\nAnswer:\",\n",
    "            \"expected\": \"POSITIVE\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"discrete\",\n",
    "            \"name\": \"Pattern completion\",\n",
    "            \"prompt\": \"Complete the sequence: A, B, C, D, ___\",\n",
    "            \"expected\": \"E\"\n",
    "        },\n",
    "        # Continuous tasks (may fail)\n",
    "        {\n",
    "            \"type\": \"continuous\",\n",
    "            \"name\": \"Novel arithmetic\",\n",
    "            \"prompt\": \"Calculate: 847 * 293 = ?\\nAnswer with just the number:\",\n",
    "            \"expected\": \"248171\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"continuous\",\n",
    "            \"name\": \"Time calculation\",\n",
    "            \"prompt\": \"If it's 9:47 AM and I wait 3 hours and 38 minutes, what time is it?\\nAnswer with just the time:\",\n",
    "            \"expected\": \"1:25 PM\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"continuous\",\n",
    "            \"name\": \"Duration comparison\",\n",
    "            \"prompt\": \"Which is longer: 2 hours 45 minutes or 175 minutes?\\nAnswer with just the one that is longer:\",\n",
    "            \"expected\": \"175 minutes\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DISCRETE VS CONTINUOUS TASK PERFORMANCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for test in tests:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=50,\n",
    "            messages=[{\"role\": \"user\", \"content\": test[\"prompt\"]}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text.strip()\n",
    "        correct = test[\"expected\"].lower() in answer.lower()\n",
    "        \n",
    "        results.append({\n",
    "            \"type\": test[\"type\"],\n",
    "            \"name\": test[\"name\"],\n",
    "            \"expected\": test[\"expected\"],\n",
    "            \"got\": answer[:50],\n",
    "            \"correct\": correct\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{test['type'].upper()}: {test['name']}\")\n",
    "        print(f\"  Expected: {test['expected']}\")\n",
    "        print(f\"  Got: {answer[:50]}\")\n",
    "        print(f\"  Correct: {'YES' if correct else 'NO'}\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Summary\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for task_type in [\"discrete\", \"continuous\"]:\n",
    "        subset = df[df[\"type\"] == task_type]\n",
    "        accuracy = subset[\"correct\"].mean()\n",
    "        print(f\"{task_type.capitalize()} tasks: {accuracy:.1%} accuracy\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "results_df = test_discrete_vs_continuous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 2: Pattern Matching vs Computation\n",
    "\n",
    "LLMs excel at pattern matching against training data. They struggle with genuine computation that requires:\n",
    "- Maintaining intermediate state\n",
    "- Applying rules systematically\n",
    "- Handling novel inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pattern_vs_computation():\n",
    "    \"\"\"Test whether model is pattern matching or computing.\"\"\"\n",
    "    \n",
    "    # Test 1: Common vs Novel arithmetic\n",
    "    arithmetic_tests = [\n",
    "        # Common (likely in training data)\n",
    "        {\"prompt\": \"What is 2 + 2?\", \"expected\": \"4\", \"type\": \"common\"},\n",
    "        {\"prompt\": \"What is 10 * 10?\", \"expected\": \"100\", \"type\": \"common\"},\n",
    "        # Novel (unlikely in training data)\n",
    "        {\"prompt\": \"What is 7847 + 2938?\", \"expected\": \"10785\", \"type\": \"novel\"},\n",
    "        {\"prompt\": \"What is 6291 - 4873?\", \"expected\": \"1418\", \"type\": \"novel\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PATTERN MATCHING VS COMPUTATION: ARITHMETIC\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for test in arithmetic_tests:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=20,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{test['prompt']} Answer with just the number.\"}]\n",
    "        )\n",
    "        answer = response.content[0].text.strip()\n",
    "        correct = test[\"expected\"] in answer\n",
    "        \n",
    "        results.append({\n",
    "            \"type\": test[\"type\"],\n",
    "            \"prompt\": test[\"prompt\"],\n",
    "            \"expected\": test[\"expected\"],\n",
    "            \"got\": answer,\n",
    "            \"correct\": correct\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{test['type'].upper()}: {test['prompt']}\")\n",
    "        print(f\"  Expected: {test['expected']}, Got: {answer}, Correct: {'YES' if correct else 'NO'}\")\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "arithmetic_df = test_pattern_vs_computation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compositional_generalization():\n",
    "    \"\"\"Test Lake & Baroni style compositional generalization.\"\"\"\n",
    "    \n",
    "    # Define primitives\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPOSITIONAL GENERALIZATION (Lake & Baroni)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Training-like examples (primitives)\n",
    "    training_prompt = \"\"\"Learn these definitions:\n",
    "- 'dax' means 'jump'\n",
    "- 'wif' means 'twice'\n",
    "- 'lug' means 'walk'\n",
    "- 'zup' means 'opposite direction'\n",
    "\n",
    "Examples:\n",
    "- 'dax' = jump\n",
    "- 'wif dax' = jump jump\n",
    "- 'lug' = walk\n",
    "\"\"\"\n",
    "    \n",
    "    # Test novel compositions\n",
    "    composition_tests = [\n",
    "        {\"input\": \"wif lug\", \"expected\": \"walk walk\", \"difficulty\": \"easy\"},\n",
    "        {\"input\": \"wif wif dax\", \"expected\": \"jump jump jump jump\", \"difficulty\": \"medium\"},\n",
    "        {\"input\": \"zup dax\", \"expected\": \"opposite of jump / jump backwards\", \"difficulty\": \"hard\"},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for test in composition_tests:\n",
    "        prompt = f\"\"\"{training_prompt}\n",
    "Now interpret: '{test['input']}'\n",
    "What actions does this represent?\"\"\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=100,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text.strip()\n",
    "        \n",
    "        print(f\"\\n{test['difficulty'].upper()}: '{test['input']}'\")\n",
    "        print(f\"  Expected: {test['expected']}\")\n",
    "        print(f\"  Got: {answer[:100]}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"difficulty\": test[\"difficulty\"],\n",
    "            \"input\": test[\"input\"],\n",
    "            \"response\": answer\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return results\n",
    "\n",
    "comp_results = test_compositional_generalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 3: Architectural Prerequisites Framework\n",
    "\n",
    "Use this framework to assess whether your task's requirements match LLM capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitecturalPrerequisiteAnalyzer:\n",
    "    \"\"\"Framework for analyzing architectural prerequisites.\"\"\"\n",
    "    \n",
    "    PREREQUISITES = {\n",
    "        \"discrete_representation\": {\n",
    "            \"description\": \"Task can be represented with discrete symbols/tokens\",\n",
    "            \"llm_provides\": True,\n",
    "            \"examples\": [\"text classification\", \"named entity recognition\", \"translation\"]\n",
    "        },\n",
    "        \"continuous_representation\": {\n",
    "            \"description\": \"Task requires continuous magnitude representation\",\n",
    "            \"llm_provides\": False,\n",
    "            \"examples\": [\"precise arithmetic\", \"physical simulation\", \"temporal duration\"]\n",
    "        },\n",
    "        \"state_maintenance\": {\n",
    "            \"description\": \"Task requires maintaining and updating state over time\",\n",
    "            \"llm_provides\": False,\n",
    "            \"examples\": [\"tracking inventory\", \"game state\", \"evolving constraints\"]\n",
    "        },\n",
    "        \"compositional_generalization\": {\n",
    "            \"description\": \"Task requires systematic combination of primitives\",\n",
    "            \"llm_provides\": False,\n",
    "            \"examples\": [\"novel algorithm design\", \"proof construction\", \"planning\"]\n",
    "        },\n",
    "        \"pattern_completion\": {\n",
    "            \"description\": \"Task involves completing familiar patterns\",\n",
    "            \"llm_provides\": True,\n",
    "            \"examples\": [\"text completion\", \"code completion\", \"style transfer\"]\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"description\": \"Task requires recalling facts from training data\",\n",
    "            \"llm_provides\": \"Partial\",\n",
    "            \"examples\": [\"factual Q&A\", \"definitions\", \"general knowledge\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def analyze(self, task_name: str, required_prerequisites: List[str]) -> Dict:\n",
    "        \"\"\"Analyze whether LLM can handle the task.\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            \"task\": task_name,\n",
    "            \"prerequisites\": [],\n",
    "            \"gaps\": [],\n",
    "            \"llm_fit\": None,\n",
    "            \"recommendation\": None\n",
    "        }\n",
    "        \n",
    "        for prereq in required_prerequisites:\n",
    "            if prereq in self.PREREQUISITES:\n",
    "                prereq_info = self.PREREQUISITES[prereq]\n",
    "                provides = prereq_info[\"llm_provides\"]\n",
    "                \n",
    "                analysis[\"prerequisites\"].append({\n",
    "                    \"name\": prereq,\n",
    "                    \"llm_provides\": provides,\n",
    "                    \"description\": prereq_info[\"description\"]\n",
    "                })\n",
    "                \n",
    "                if provides == False:\n",
    "                    analysis[\"gaps\"].append(prereq)\n",
    "        \n",
    "        # Determine overall fit\n",
    "        if len(analysis[\"gaps\"]) == 0:\n",
    "            analysis[\"llm_fit\"] = \"Good\"\n",
    "            analysis[\"recommendation\"] = \"LLM-only approach likely sufficient. Proceed with testing.\"\n",
    "        elif len(analysis[\"gaps\"]) == 1:\n",
    "            analysis[\"llm_fit\"] = \"Partial\"\n",
    "            analysis[\"recommendation\"] = f\"Consider hybrid architecture to address: {analysis['gaps'][0]}\"\n",
    "        else:\n",
    "            analysis[\"llm_fit\"] = \"Poor\"\n",
    "            analysis[\"recommendation\"] = f\"Hybrid architecture required. Gaps: {', '.join(analysis['gaps'])}\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def print_analysis(self, analysis: Dict):\n",
    "        \"\"\"Print formatted analysis.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ARCHITECTURAL ANALYSIS: {analysis['task']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\nRequired Prerequisites:\")\n",
    "        for prereq in analysis[\"prerequisites\"]:\n",
    "            status = \"YES\" if prereq[\"llm_provides\"] == True else (\"PARTIAL\" if prereq[\"llm_provides\"] == \"Partial\" else \"NO\")\n",
    "            print(f\"  - {prereq['name']}: LLM provides = {status}\")\n",
    "        \n",
    "        print(f\"\\nArchitectural Gaps: {analysis['gaps'] if analysis['gaps'] else 'None'}\")\n",
    "        print(f\"LLM Fit: {analysis['llm_fit']}\")\n",
    "        print(f\"\\nRecommendation: {analysis['recommendation']}\")\n",
    "\n",
    "\n",
    "# Example analyses\n",
    "analyzer = ArchitecturalPrerequisiteAnalyzer()\n",
    "\n",
    "# Example 1: Content summarization (good fit)\n",
    "analysis1 = analyzer.analyze(\n",
    "    \"Document Summarization\",\n",
    "    [\"discrete_representation\", \"pattern_completion\"]\n",
    ")\n",
    "analyzer.print_analysis(analysis1)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 2: Medical scheduling (poor fit)\n",
    "analysis2 = analyzer.analyze(\n",
    "    \"Medical Appointment Scheduling\",\n",
    "    [\"continuous_representation\", \"state_maintenance\", \"pattern_completion\"]\n",
    ")\n",
    "analyzer.print_analysis(analysis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 4: Biological Existence Proofs\n",
    "\n",
    "Nature demonstrates that specialized mechanisms are needed for temporal reasoning. These are NOT pattern matching systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize biological timing mechanisms\n",
    "\n",
    "biological_systems = {\n",
    "    \"System\": [\n",
    "        \"Hippocampal Time Cells\",\n",
    "        \"Cerebellar Timing\",\n",
    "        \"Interval Timing (Dopaminergic)\",\n",
    "        \"Circadian Rhythms\"\n",
    "    ],\n",
    "    \"Time Scale\": [\n",
    "        \"Seconds to minutes\",\n",
    "        \"Milliseconds\",\n",
    "        \"Seconds to hours\",\n",
    "        \"~24 hours\"\n",
    "    ],\n",
    "    \"Mechanism\": [\n",
    "        \"Sequential neural firing\",\n",
    "        \"Parallel fiber delays\",\n",
    "        \"Dopamine accumulation\",\n",
    "        \"Transcription-translation loops\"\n",
    "    ],\n",
    "    \"Pattern Matching?\": [\n",
    "        \"No - computational\",\n",
    "        \"No - physical delay lines\",\n",
    "        \"No - accumulator model\",\n",
    "        \"No - molecular oscillator\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_bio = pd.DataFrame(biological_systems)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BIOLOGICAL TIMING MECHANISMS: Existence Proofs for Specialized Processing\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(df_bio.to_string(index=False))\n",
    "print()\n",
    "print(\"Key Insight: None of these systems work by pattern matching on symbolic\")\n",
    "print(\"representations. They use dedicated computational mechanisms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 5: Exercise - Analyze Your Deployment Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXERCISE: Analyze your deployment scenario\n",
    "\n",
    "analyzer = ArchitecturalPrerequisiteAnalyzer()\n",
    "\n",
    "# Fill in your scenario\n",
    "my_analysis = analyzer.analyze(\n",
    "    \"YOUR TASK NAME HERE\",  # e.g., \"Customer Support Chatbot\"\n",
    "    [\n",
    "        # Uncomment the prerequisites your task requires:\n",
    "        \"discrete_representation\",\n",
    "        \"pattern_completion\",\n",
    "        # \"continuous_representation\",\n",
    "        # \"state_maintenance\",\n",
    "        # \"compositional_generalization\",\n",
    "        # \"retrieval\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "analyzer.print_analysis(my_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs operate on discrete tokens.** Tasks requiring continuous magnitude representation will fail.\n",
    "\n",
    "2. **Pattern matching ≠ computation.** High accuracy on common examples doesn't guarantee accuracy on novel inputs.\n",
    "\n",
    "3. **Biology provides existence proofs.** Reliable temporal reasoning requires specialized mechanisms, not pattern matching.\n",
    "\n",
    "4. **Analyze prerequisites before building.** Understanding architectural fit prevents wasted effort.\n",
    "\n",
    "5. **Gaps indicate hybrid architecture needs.** When LLMs can't provide a prerequisite, add specialized components.\n",
    "\n",
    "---\n",
    "\n",
    "**Homework:** Complete the architectural analysis for your deployment scenario.\n",
    "\n",
    "**Next Session:** Experimental Design I—Constructing Diagnostic Scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
