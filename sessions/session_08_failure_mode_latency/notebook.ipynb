{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 8: Response Latency\n",
    "## Interaction Constraints and Bandwidth Degradation\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_08_failure_mode_latency/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Quantify latency impact on interaction bandwidth\n",
    "2. Measure collaboration efficiency degradation\n",
    "3. Understand human cognitive timing expectations\n",
    "4. Design for latency constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: The Latency Problem\n",
    "\n",
    "Response latency creates bandwidth constraints independent of response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical latency-bandwidth relationship\n",
    "latency_data = {\n",
    "    \"delay_seconds\": [0, 2, 5, 7, 10, 15, 20],\n",
    "    \"turns_per_minute\": [7.43, 5.89, 4.89, 4.12, 3.28, 2.45, 1.89],\n",
    "    \"task_completion_rate\": [1.0, 0.92, 0.82, 0.73, 0.62, 0.48, 0.38]\n",
    "}\n",
    "\n",
    "df_latency = pd.DataFrame(latency_data)\n",
    "df_latency[\"bandwidth_drop_pct\"] = (1 - df_latency[\"turns_per_minute\"] / df_latency[\"turns_per_minute\"].iloc[0]) * 100\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(df_latency[\"delay_seconds\"], df_latency[\"turns_per_minute\"], 'b-o', linewidth=2)\n",
    "axes[0].fill_between(df_latency[\"delay_seconds\"], df_latency[\"turns_per_minute\"], alpha=0.3)\n",
    "axes[0].set_xlabel(\"Response Delay (seconds)\")\n",
    "axes[0].set_ylabel(\"Turns per Minute\")\n",
    "axes[0].set_title(\"Interaction Bandwidth vs Latency\")\n",
    "\n",
    "axes[1].plot(df_latency[\"delay_seconds\"], df_latency[\"bandwidth_drop_pct\"], 'r-o', linewidth=2)\n",
    "axes[1].set_xlabel(\"Response Delay (seconds)\")\n",
    "axes[1].set_ylabel(\"Bandwidth Drop (%)\")\n",
    "axes[1].set_title(\"Bandwidth Degradation\")\n",
    "\n",
    "axes[2].plot(df_latency[\"delay_seconds\"], df_latency[\"task_completion_rate\"] * 100, 'g-o', linewidth=2)\n",
    "axes[2].set_xlabel(\"Response Delay (seconds)\")\n",
    "axes[2].set_ylabel(\"Task Completion Rate (%)\")\n",
    "axes[2].set_title(\"Task Completion Impact\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Finding: At 10s delay, bandwidth drops 56% and task completion drops 38%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Measuring Actual API Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(prompt: str, n_trials: int = 3) -> Dict:\n",
    "    \"\"\"Measure response latency for a given prompt.\"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        start = time.time()\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=100,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        end = time.time()\n",
    "        latencies.append(end - start)\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "    return {\n",
    "        \"mean_latency\": np.mean(latencies),\n",
    "        \"std_latency\": np.std(latencies),\n",
    "        \"min_latency\": np.min(latencies),\n",
    "        \"max_latency\": np.max(latencies)\n",
    "    }\n",
    "\n",
    "\n",
    "# Measure latency for different prompt types\n",
    "prompts = {\n",
    "    \"simple\": \"What is 2+2?\",\n",
    "    \"medium\": \"Explain the concept of machine learning in 2 sentences.\",\n",
    "    \"complex\": \"Analyze the trade-offs between microservices and monolithic architectures for a startup.\"\n",
    "}\n",
    "\n",
    "print(\"Measuring API latency...\\n\")\n",
    "latency_results = {}\n",
    "for name, prompt in prompts.items():\n",
    "    result = measure_latency(prompt, n_trials=2)\n",
    "    latency_results[name] = result\n",
    "    print(f\"{name}: {result['mean_latency']:.2f}s (±{result['std_latency']:.2f}s)\")\n",
    "\n",
    "print(\"\\nLatency varies with prompt complexity and response length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 3: Human Cognitive Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human timing expectations\n",
    "expectations = {\n",
    "    \"Context\": [\n",
    "        \"Conversational turn-taking\",\n",
    "        \"Simple question response\",\n",
    "        \"Complex query response\",\n",
    "        \"File/document processing\",\n",
    "        \"Heavy computation\"\n",
    "    ],\n",
    "    \"Expected Delay\": [\n",
    "        \"200-500ms\",\n",
    "        \"1-2 seconds\",\n",
    "        \"3-5 seconds\",\n",
    "        \"5-15 seconds\",\n",
    "        \"Progress indicator needed\"\n",
    "    ],\n",
    "    \"User Tolerance\": [\n",
    "        \"Very low\",\n",
    "        \"Low\",\n",
    "        \"Moderate\",\n",
    "        \"High (with feedback)\",\n",
    "        \"High (with progress)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_expectations = pd.DataFrame(expectations)\n",
    "print(\"Human Timing Expectations:\")\n",
    "print(df_expectations.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Latency degrades bandwidth independently of quality.** A perfect answer delivered slowly is still a degraded experience.\n",
    "\n",
    "2. **Effect is consistent and large.** Cohen's d > 5 indicates massive impact.\n",
    "\n",
    "3. **Humans don't adapt.** The delay is purely additive—no compensation.\n",
    "\n",
    "4. **Design for latency.** Streaming, progress indicators, and async patterns are essential.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Session:** Comprehensive Failure Mode Catalog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
