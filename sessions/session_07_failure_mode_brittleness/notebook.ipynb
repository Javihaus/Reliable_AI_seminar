{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Session 7: Prompt Brittleness\n",
    "## When Robust Understanding Fails\n",
    "\n",
    "**Production LLM Deployment: Risk Characterization Before Failure**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/Production_LLM_Deployment/blob/main/sessions/session_07_failure_mode_brittleness/notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Use brittleness as an architectural diagnostic\n",
    "2. Design semantically equivalent prompt variations\n",
    "3. Quantify brittleness with statistical rigor\n",
    "4. Interpret brittleness across domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anthropic numpy pandas matplotlib seaborn scipy\n",
    "\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    import os\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Why Brittleness Matters\n",
    "\n",
    "If a model truly understands a task, its answer shouldn't change based on irrelevant surface features. Brittleness = pattern matching evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BrittlenessTest:\n",
    "    \"\"\"A test scenario with multiple format variations.\"\"\"\n",
    "    name: str\n",
    "    ground_truth: str\n",
    "    formats: Dict[str, str]  # format_name -> prompt\n",
    "\n",
    "\n",
    "def run_brittleness_test(test: BrittlenessTest, client) -> Dict:\n",
    "    \"\"\"Run all format variations and calculate brittleness.\"\"\"\n",
    "    results = {\"name\": test.name, \"ground_truth\": test.ground_truth, \"format_results\": {}}\n",
    "    \n",
    "    for fmt_name, prompt in test.formats.items():\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            max_tokens=20,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text.strip().upper()\n",
    "        if \"YES\" in answer[:5]:\n",
    "            answer = \"YES\"\n",
    "        elif \"NO\" in answer[:5]:\n",
    "            answer = \"NO\"\n",
    "        else:\n",
    "            answer = \"UNCLEAR\"\n",
    "        \n",
    "        results[\"format_results\"][fmt_name] = {\n",
    "            \"answer\": answer,\n",
    "            \"correct\": answer == test.ground_truth\n",
    "        }\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracies = [1 if r[\"correct\"] else 0 for r in results[\"format_results\"].values()]\n",
    "    results[\"accuracy\"] = np.mean(accuracies)\n",
    "    results[\"brittleness\"] = (max(accuracies) - min(accuracies)) * 100\n",
    "    results[\"consistent\"] = len(set(r[\"answer\"] for r in results[\"format_results\"].values())) == 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-format test scenarios\n",
    "brittleness_tests = [\n",
    "    BrittlenessTest(\n",
    "        name=\"Medication Timing (Unsafe)\",\n",
    "        ground_truth=\"NO\",\n",
    "        formats={\n",
    "            \"natural\": \"I took aspirin at 8:00 AM. Can I take ibuprofen at 11:00 AM if I need to wait 4 hours? YES or NO only.\",\n",
    "            \"clinical\": \"MEDICATION CHECK\\nDrug A: Aspirin @ 0800\\nDrug B: Ibuprofen requested @ 1100\\nRequired interval: 4h\\nSafe to administer? YES/NO\",\n",
    "            \"json\": '{\"med1\": {\"drug\": \"aspirin\", \"time\": \"8:00 AM\"}, \"med2\": {\"drug\": \"ibuprofen\", \"time\": \"11:00 AM\"}, \"min_gap_hours\": 4}\\nIs this safe? YES or NO',\n",
    "            \"conversational\": \"hey so i took aspirin around 8 this morning and its 11 now. supposed to wait 4 hrs before ibuprofen. good to go? just yes or no\",\n",
    "            \"formal\": \"Query: Temporal constraint verification\\nEvent A: Aspirin administration at t=08:00\\nEvent B: Ibuprofen administration proposed at t=11:00\\nConstraint: Minimum interval of 4 hours required\\nIs constraint satisfied? Respond YES or NO.\"\n",
    "        }\n",
    "    ),\n",
    "    BrittlenessTest(\n",
    "        name=\"Medication Timing (Safe)\",\n",
    "        ground_truth=\"YES\",\n",
    "        formats={\n",
    "            \"natural\": \"I took aspirin at 8:00 AM. Can I take ibuprofen at 1:00 PM if I need to wait 4 hours? YES or NO only.\",\n",
    "            \"clinical\": \"MEDICATION CHECK\\nDrug A: Aspirin @ 0800\\nDrug B: Ibuprofen requested @ 1300\\nRequired interval: 4h\\nSafe to administer? YES/NO\",\n",
    "            \"json\": '{\"med1\": {\"drug\": \"aspirin\", \"time\": \"8:00 AM\"}, \"med2\": {\"drug\": \"ibuprofen\", \"time\": \"1:00 PM\"}, \"min_gap_hours\": 4}\\nIs this safe? YES or NO',\n",
    "            \"conversational\": \"hey so i took aspirin around 8 this morning and its 1 pm now. supposed to wait 4 hrs before ibuprofen. good to go? just yes or no\",\n",
    "            \"formal\": \"Query: Temporal constraint verification\\nEvent A: Aspirin administration at t=08:00\\nEvent B: Ibuprofen administration proposed at t=13:00\\nConstraint: Minimum interval of 4 hours required\\nIs constraint satisfied? Respond YES or NO.\"\n",
    "        }\n",
    "    ),\n",
    "    BrittlenessTest(\n",
    "        name=\"Meeting Overlap (Yes)\",\n",
    "        ground_truth=\"YES\",\n",
    "        formats={\n",
    "            \"natural\": \"Meeting A: 2pm-4pm. Meeting B: 3pm-5pm. Do they overlap? YES or NO only.\",\n",
    "            \"formal\": \"Interval A: [14:00, 16:00]\\nInterval B: [15:00, 17:00]\\nDo these intervals intersect? YES/NO\",\n",
    "            \"conversational\": \"got a meeting from 2 to 4 and another from 3 to 5. conflict? yes or no\",\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "all_results = []\n",
    "for test in brittleness_tests:\n",
    "    result = run_brittleness_test(test, client)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test: {result['name']}\")\n",
    "    print(f\"Ground Truth: {result['ground_truth']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for fmt, res in result[\"format_results\"].items():\n",
    "        status = \"CORRECT\" if res[\"correct\"] else \"WRONG\"\n",
    "        print(f\"  {fmt}: {res['answer']} - {status}\")\n",
    "    print(f\"\\nAccuracy: {result['accuracy']:.1%}\")\n",
    "    print(f\"Brittleness: {result['brittleness']:.0f} pp\")\n",
    "    print(f\"Consistent: {'Yes' if result['consistent'] else 'No (BRITTLE)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATE BRITTLENESS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "avg_brittleness = np.mean([r[\"brittleness\"] for r in all_results])\n",
    "max_brittleness = max([r[\"brittleness\"] for r in all_results])\n",
    "consistent_count = sum(1 for r in all_results if r[\"consistent\"])\n",
    "\n",
    "print(f\"\\nAverage brittleness: {avg_brittleness:.1f} pp\")\n",
    "print(f\"Maximum brittleness: {max_brittleness:.0f} pp\")\n",
    "print(f\"Consistent tests: {consistent_count}/{len(all_results)}\")\n",
    "\n",
    "if avg_brittleness > 30:\n",
    "    print(\"\\nDIAGNOSIS: High brittleness indicates pattern matching, not robust understanding.\")\n",
    "elif avg_brittleness > 10:\n",
    "    print(\"\\nDIAGNOSIS: Moderate brittleness - some format sensitivity detected.\")\n",
    "else:\n",
    "    print(\"\\nDIAGNOSIS: Low brittleness - relatively robust understanding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Brittleness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Brittleness by test\n",
    "test_names = [r[\"name\"][:20] for r in all_results]\n",
    "brittleness_scores = [r[\"brittleness\"] for r in all_results]\n",
    "colors = ['red' if b > 50 else 'orange' if b > 20 else 'green' for b in brittleness_scores]\n",
    "\n",
    "axes[0].barh(test_names, brittleness_scores, color=colors)\n",
    "axes[0].axvline(30, color='red', linestyle='--', label='High brittleness threshold')\n",
    "axes[0].set_xlabel('Brittleness (percentage points)')\n",
    "axes[0].set_title('Brittleness by Test Scenario')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy by format (aggregated)\n",
    "format_accuracies = {}\n",
    "for result in all_results:\n",
    "    for fmt, res in result[\"format_results\"].items():\n",
    "        if fmt not in format_accuracies:\n",
    "            format_accuracies[fmt] = []\n",
    "        format_accuracies[fmt].append(1 if res[\"correct\"] else 0)\n",
    "\n",
    "formats = list(format_accuracies.keys())\n",
    "accuracies = [np.mean(format_accuracies[f]) * 100 for f in formats]\n",
    "\n",
    "axes[1].bar(formats, accuracies, color='steelblue')\n",
    "axes[1].axhline(50, color='gray', linestyle='--', label='Random chance')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy by Prompt Format')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Brittleness reveals pattern matching.** Large accuracy swings from format changes = not robust understanding.\n",
    "\n",
    "2. **Test multiple formats always.** Never trust single-format accuracy.\n",
    "\n",
    "3. **Quantify and report brittleness.** It's a key metric for deployment decisions.\n",
    "\n",
    "4. **High brittleness suggests hybrid needed.** If format matters too much, LLM alone isn't enough.\n",
    "\n",
    "---\n",
    "\n",
    "**Homework:** Create a brittleness report for your deployment domain.\n",
    "\n",
    "**Next Session:** Failure Mode IVâ€”Response Latency and Interaction Constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
